---
title: Prompt token budgeting
description: Shape prompts, count tokens, and trim context before calling an LLM.
hero:
  title: 'LLM input shaping'
sidebar:
  order: 2
---

import { Tabs, TabItem } from '@astrojs/starlight/components';
import { Code } from '@astrojs/starlight/components';
import { getCode } from '../../../../../lib/code-snippets';

export const run = getCode("data_transforms/prompt_token_budgeting/run_prompt_token_budget.ts");
export const budget = getCode("data_transforms/prompt_token_budgeting/token_budget.ts");

## What is this about

This example shows a practical preflight step before sending LLM requests:

1. Build a prompt from a stable system prefix, history, and query.
2. Count tokens with `tiktoken`.
3. Trim old history (and query if needed) to stay inside a budget.
4. Return prompt metadata you can use for cost and caching analysis.

## Install

:::info
Bun
```bash
bun add tiktoken openai
```
:::

`openai` is optional here. The runnable example focuses on token budgeting and
does not call the API.

## Commands

:::info
Bun
```bash
src/run_prompt_token_budget.ts --threads 2 --requests 20000 --maxInputTokens 900 --model gpt-4o-mini --mode knitting
```
Deno
```bash
deno run -A src/run_prompt_token_budget.ts --threads 2 --requests 20000 --maxInputTokens 900 --model gpt-4o-mini --mode knitting
```

Node
```bash
npx tsx src/run_prompt_token_budget.ts --threads 2 --requests 20000 --maxInputTokens 900 --model gpt-4o-mini --mode knitting
```

Host baseline (no workers)
```bash
src/run_prompt_token_budget.ts --requests 20000 --maxInputTokens 900 --model gpt-4o-mini --mode host
```
:::

## What happens in this example

1. The host generates many prompt inputs (same system prefix, different histories/queries).
2. Each task builds a prompt and counts tokens with `tiktoken`.
3. If over budget, it trims oldest turns first, then truncates the query as a last step.
4. The host aggregates token savings and trim counts.

## Code

<Tabs>
  <TabItem label="run_prompt_token_budget.ts">
    <Code code={run} lang="ts" title={"run_prompt_token_budget.ts"} />
  </TabItem>
  <TabItem label="token_budget.ts">
    <Code code={budget} lang="ts" title={"token_budget.ts"} />
  </TabItem>
</Tabs>

## Why this pattern matters

- Lower token usage can reduce cost and latency.
- Stable prefixes improve prompt-cache hit rates on repeated requests.
- Budgeting logic keeps requests predictable before they hit the model.
