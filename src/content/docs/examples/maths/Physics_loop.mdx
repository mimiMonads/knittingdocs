---
title: Physics loop
description: "Parallel random-walk first-exit simulation"
hero:
  title: 'Parallel Monte Carlo Physics Loop'
sidebar:
  order: 3

---

import { Tabs, TabItem } from '@astrojs/starlight/components';
import { Code } from '@astrojs/starlight/components';
import { getCode } from '../../../../lib/code-snippets';

export const run = getCode("maths/physics_style_loops/walks_runs.ts");
export const walk_chunk = getCode("maths/physics_style_loops/walk2d.ts");

## What is this about

This example runs a **physics-style simulation loop** (tight, step-by-step state updates) and parallelizes it with **Knitting’s worker pool**.

Instead of a pure math kernel (like estimating π), we simulate a process through time steps: update state, check conditions, repeat.

Here the “physics” is a simple **2D random walk / diffusion toy model**:

* start at `(0,0)`
* at each step, move one unit in a random direction
* stop when the particle first crosses a radius boundary

We measure:

* **escape probability** (did it hit the boundary within `maxSteps`?)
* **time to escape** (how many steps it took, for runs that escaped)

---

## What happens in this example

1. The host creates a **pool** with `createPool`.
2. The simulation is split into **chunks** (jobs). Each job runs many independent particle trials.
3. Each worker executes a **tight inner loop**:

   * initialize particle state `(x, y)`
   * for each step:

     * choose a random direction
     * update `(x, y)`
     * test escape condition (radius reached)
4. Each chunk returns **compact summary stats**:

   * number of escaped particles
   * total runs in this chunk
   * sum of escape steps (and optionally sum of squares)
5. The host aggregates chunk results into final estimates.

This is the standard scientific pattern:

* **map**: simulate many independent trials
* **reduce**: combine partial statistics into global results

---

## Commands

:::info
Bun
```bash
src/walks_runs.ts --threads 6 --runs 15000000 --batch 5000 --steps 15000 --radius 100
```
Deno
```bash
deno run -A src/walks_runs.ts --threads 6 --runs 15000000 --batch 5000 --steps 15000 --radius 100
```

Node
```bash
npx tsx src/walks_runs.ts --threads 6 --runs 15000000 --batch 5000 --steps 15000 --radius 100
```
:::

## Code
<Tabs>
  <TabItem label="walks_runs.ts">
    <Code code={run} lang="ts" title={"walks_runs.ts"} />
  </TabItem>
  <TabItem label="walk2d.ts">
    <Code code={walk_chunk} lang="ts" title={"walk2d.ts"} />
  </TabItem>
</Tabs>

---

## What is this good for

Physics-style Monte Carlo is useful when:

* dynamics are too complex for closed-form solutions
* you need probabilities, expected times, or distributions
* your system is stochastic (randomness is part of the model)

Real use cases:

* **diffusion / Brownian motion** (escape times, mean squared displacement)
* **hitting time problems** in stochastic processes
* **Monte Carlo transport** (particles moving through materials)
* **agent-based models** (crowd, traffic, epidemics)
* **game simulation** (probabilistic outcomes, balancing, AI)
* **uncertainty propagation** through a simulation pipeline

If your problem is “simulate it many times and average the outcome”, this structure fits.

---

## Scientific background

### What Monte Carlo is estimating

We run repeated experiments and estimate quantities like:

**Escape probability**
$$
\hat{p} = \frac{\text{escaped}}{\text{total runs}}
$$

**Mean steps to escape** (conditional on escape)
$$
\hat{\mu} = \frac{\sum \text{steps}}{\text{escaped}}
$$

**Variance / spread**
$$
\widehat{\mathrm{Var}} = \frac{\sum \text{steps}^2}{\text{escaped}} - \hat{\mu}^2
$$

### Why it converges

As the number of independent runs $$N$$ increases, estimates converge by the **Law of Large Numbers**.

### How error scales

Monte Carlo accuracy usually improves like:
$$
\text{error} \propto \frac{1}{\sqrt{N}}
$$
So reducing error by 2× costs about 4× more runs.

### “Physics loop” note

This simulation is a discrete-time approximation of a continuous stochastic process. Smaller step sizes and more steps can approximate continuous diffusion better, but cost more compute.

---

## How parallelization works here

The simulation is **embarrassingly parallel**:

* Each particle run is independent (given its RNG stream)
* We split runs across workers
* Each worker returns a **small summary** instead of per-step trajectories

This is important: returning full paths would waste bandwidth and memory. For Monte Carlo, you usually only need aggregated statistics.

---

## Lessons on using the scripts

### 1) Chunk size matters

Pick a chunk size that amortizes overhead:

* too small → dispatch overhead dominates
* too large → worse load balance and slower feedback

A good starting point is something like:

* `--batch 5k to 50k` for heavier loops
* increase if each run is short

### 2) Prefer batch dispatch

Queue many `call`s, then await them together. This reduces scheduling overhead and tends to improve throughput.

### 3) Use deterministic RNG and seed properly

Scientific simulations need reproducibility:

* keep your scenario parameters fixed (`--radius`, `--steps`, direction table size)
* use one base seed per run
* derive a different per-job seed so chunks don’t share the same random stream

### 4) Keep the inner loop “boringly tight”

Performance comes from avoiding overhead inside the physics loop:

* avoid allocations per step
* avoid creating objects per iteration
* precompute direction tables if possible
* use simple numeric types

### 5) Validate with invariants

Always check:

* `0 <= escaped <= totalRuns`
* totals add up exactly across chunks
* results are stable under fixed seeds

---

## Performance notes

### When you’ll see speedups

* many runs, each with many steps
* little to no IO
* chunk results are small
* you have multiple physical cores

### Common bottlenecks

* too many tiny tasks
* heavy math inside each step (e.g., trig every step)
* returning large data payloads
* RNG overhead (solve by using fast deterministic RNG and precomputed directions)

---

## What students should notice

* Physics loops are **branch-heavy**: escape checks introduce branching and early exits.
* Work per run is variable (some particles escape early, others take longer). Chunking smooths this out.
* The results improve with more runs, but error shrinks slowly $$\left(1/\sqrt{N}\right)$$.

---

## Suggested student exercises

1. Increase `radius` and observe how mean escape steps changes.
2. Compare different `--batch` chunk sizes and measure throughput.
3. Add variance reporting and compute a 95% confidence interval.
4. Replace random walk with a drift term (constant force) and compare escape behavior.
5. Test reproducibility: fix seeds and confirm identical outputs.
