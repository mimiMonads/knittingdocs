---
title: "Knitting"
description: "Knitting is shared-memory IPC for Node.js, Deno, and Bun, built for low-latency worker tasks and high-throughput parallel JavaScript."
template: splash
head:
  - tag: meta
    attrs:
      name: keywords
      content: "shared-memory IPC, Node.js worker threads, Deno workers, Bun workers, JavaScript parallelism, Knitting"
  - tag: meta
    attrs:
      property: og:type
      content: website
  - tag: meta
    attrs:
      name: robots
      content: "index,follow,max-image-preview:large"

hero:
  image:
    alt: A glittering, brightly colored logo
    dark: ../../assets/logo.svg
    light: ../../assets/light_logo.svg
  tagline: "Real nanosecond responses for JavaScript."
  actions:
    - text: Welcome
      link: /start/welcome/
    - text: Benchmarks
      link: /benchmarks/introduction/
---

import AshesBackground from "../../components/AshesBackground.astro";
import { Card, CardGrid, FileTree } from "@astrojs/starlight/components";
import { Tabs, TabItem } from "@astrojs/starlight/components";
import nodeIpc from "../../assets/charts/node_ipc.png";
import denoIpc from "../../assets/charts/deno_ipc.png";
import bunIpc from "../../assets/charts/bun_ipc.png";
import BenchmarkBalls from "../../components/BenchmarkBalls.astro";

<AshesBackground />

<BenchmarkBalls
  sourceLabel="based on node.js ipc graph (50 msg)"
  items={[
    { label: "Knitting", speed: 2030869, color: "#336244ff" },
    { label: "PostMessage", speed: 573394, color: "#58523bff" },
    { label: "WebSockets", speed: 178603, color: "#A78BFA" },
    { label: "HTTP", speed: 17483,  color: "#60A5FA" },
  ]}
/>
<div class="home-cards">
  <CardGrid stagger>
    <Card title="Multi-threading in 10 lines">
      Start here if you want the smallest possible mental model.
      Define tasks, create a pool, call it, and shut it down.

      ```ts
      import { isMain, task } from "@vixeny/knitting";

      export const world = task({
        f: (args: string) => args + " world",
      }).createPool({
        threads: 2,
      });

      if (isMain) {
        world.call("hello")
          .then(console.log)
          .finally(world.shutdown);
      }
      ```


      This is intentionally simple. You can add batching and balancing later.
    </Card>

    <Card title="How fast?" >
      A quick look at runtime behavior across Bun, Deno, and Node.
      Switch tabs to compare how each runtime responds under the same benchmark shape.

      <Tabs>
        <TabItem label="Node">
          <img
            src={nodeIpc.src}
            width={nodeIpc.width}
            height={nodeIpc.height}
            alt="Node IPC benchmark"
            loading="lazy"
          />
        </TabItem>
        <TabItem label="Bun">
          <img
            src={bunIpc.src}
            width={bunIpc.width}
            height={bunIpc.height}
            alt="Bun IPC benchmark"
            loading="lazy"
          />
        </TabItem>
        <TabItem label="Deno">
          <img
            src={denoIpc.src}
            width={denoIpc.width}
            height={denoIpc.height}
            alt="Deno IPC benchmark"
            loading="lazy"
          />
        </TabItem>
      </Tabs>

      Use this as orientation, then check the benchmark pages for details.
    </Card>

    <Card title="Throughput">
      These are end-to-end results for `1 MB` payloads with a full echo path:
      host writes `1 MB` into the worker, then the worker writes `1 MB` back to host.
      That means each call creates two `1 MB` objects and moves `2 MB` total.
      To keep this conservative and non-misleading, the table below shows
      **one-way equivalent throughput** (half of roundtrip GB/s).

      Even with that strict view, Bun still delivers double-digit GB/s.

      **Hot scoreboard (GB/s, one-way equivalent):**

      | Runtime | `Uint8Array` | `string` |
      | --- | ---: | ---: |
      | **Bun** | **11.93** | **9.74** |
      | **Deno** | **1.17** | **2.68** |
      | **Node** | **1.15** | **1.32** |


      [Benchmarks overview](/benchmarks/introduction/)
   
    </Card>



    <Card title="One more thread, big impact">
      A common pattern: offload the expensive path (SSR, parsing, crypto, compression) to a worker so the main
      route loop stays responsive under mixed load.

      This is an 80/20 problem: a small fraction of work often dominates latency and throughput. Fix the dominant
      path first, then measure again.

      Taguchi-style thinking helps here: run small, controlled experiments (same request mix, same concurrency),
      change one factor (like worker threads), and pick the setting that is robust across noise (GC, CPU contention,
      bursty traffic).

      Try it in the Hono server example:
      - [Hono + SSR + JWT](/examples/data_transforms/rendering_output/Hono_server)
    </Card>

        <Card title="10+ diverse examples (and growing)">
      There is enough here to copy real patterns, not just toy snippets.
      We keep adding examples as new workflows land.

      <FileTree>
      - examples/
        - intro_examples.mdx
        - data_transforms/
          - intro_data_transforms.mdx
          - rendering_output/
            - React_ssr.mdx
            - Hono_server.mdx
            - …
          - validation/
            - Schema_validate.mdx
            - Jwt_revalidation.mdx
            - …
        - maths/
          - Big_prime.mdx
          - Monte_pi.mdx
          - Physics_loop.mdx
          - …
      </FileTree>

      - [Browse examples](/examples/intro_examples)

    We are fully documentated!
    </Card>

    <Card title="Flexible by default">
      Built to keep agent-style workflows simple and adaptable.

      - Promise inputs and async outputs work naturally.
      - Rich payload support plus explicit serialization behavior.
      - Tuning knobs for timeout, balancing, inlining, and dispatch.


    </Card>


    <Card title="Tune for workload shape">
      Knitting exposes the knobs you usually need in production.

      - Balance lanes with `balancer` strategies.
      - Add host execution with `inliner` when it helps.
      - Control spin/park/backoff/timeout and more.

    </Card>
  </CardGrid>
</div>


## Why Knitting

- Function-call workflow instead of manual message routing.
- Lower IPC overhead for high-frequency workloads.
- Same model across Node, Deno, and Bun.
- Explicit control over batching, inlining, and shutdown behavior.


## Project Scope

Knitting focuses on local multi-thread execution with shared-memory IPC.
It is not a distributed scheduler or cluster manager.
