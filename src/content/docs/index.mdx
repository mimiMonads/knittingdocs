---
title: "Knitting"
description: "Knitting is shared-memory IPC for Node.js, Deno, and Bun, built for low-latency worker tasks and high-throughput parallel JavaScript."
template: splash
head:
  - tag: meta
    attrs:
      name: keywords
      content: "shared-memory IPC, Node.js worker threads, Deno workers, Bun workers, JavaScript parallelism, Knitting"
  - tag: meta
    attrs:
      property: og:type
      content: website
  - tag: meta
    attrs:
      name: robots
      content: "index,follow,max-image-preview:large"

hero:
  image:
    alt: A glittering, brightly colored logo
    dark: ../../assets/logo.svg
    light: ../../assets/light_logo.svg
  tagline: "Real nanosecond responses for JavaScript."
  actions:
    - text: Welcome
      link: /start/welcome/
    - text: Benchmarks
      link: /benchmarks/introduction/
---

import AshesBackground from "../../components/AshesBackground.astro";
import { Card, CardGrid } from "@astrojs/starlight/components";
import { Tabs, TabItem } from "@astrojs/starlight/components";
import nodeIpc from "../../assets/charts/node_ipc.png";
import denoIpc from "../../assets/charts/deno_ipc.png";
import bunIpc from "../../assets/charts/bun_ipc.png";
import BenchmarkBalls from "../../components/BenchmarkBalls.astro";

<AshesBackground />

<BenchmarkBalls
  sourceLabel="based on node.js ipc graph (50 msg)"
  items={[
    { label: "Knitting", speed: 2030869, color: "#336244ff" },
    { label: "PostMessage", speed: 573394, color: "#58523bff" },
    { label: "WebSockets", speed: 178603, color: "#A78BFA" },
    { label: "HTTP", speed: 17483,  color: "#60A5FA" },
  ]}
/>
<CardGrid stagger>
  <Card title="Multi-threading in 10 lines">
    Start here if you want the smallest possible mental model.
    Define tasks, create a pool, call it, and shut it down.

    ```ts
    import { isMain, task } from "@vixeny/knitting";

    export const world = task({
      f: (args: string) => args + " world",
    }).createPool({
      threads: 2,
    });

    if (isMain) {
      world.call("hello")
        .then(console.log)
        .finally(world.shutdown);
    }
    ```


    This is intentionally simple. You can add batching and balancing later.
  </Card>

  <Card title="How fast?" >
    A quick look at runtime behavior across Bun, Deno, and Node.
    Switch tabs to compare how each runtime responds under the same benchmark shape.

    <Tabs>
      <TabItem label="Node">
        <img
          src={nodeIpc.src}
          width={nodeIpc.width}
          height={nodeIpc.height}
          alt="Node IPC benchmark"
          loading="lazy"
        />
      </TabItem>
      <TabItem label="Bun">
        <img
          src={bunIpc.src}
          width={bunIpc.width}
          height={bunIpc.height}
          alt="Bun IPC benchmark"
          loading="lazy"
        />
      </TabItem>
      <TabItem label="Deno">
        <img
          src={denoIpc.src}
          width={denoIpc.width}
          height={denoIpc.height}
          alt="Deno IPC benchmark"
          loading="lazy"
        />
      </TabItem>
    </Tabs>

    Use this as orientation, then check the benchmark pages for details.
  </Card>

  <Card title="10+ diverse examples (and growing)">
    There is enough here to copy real patterns, not just toy snippets.
    We keep adding examples as new workflows land.

    - [Browse examples](/examples/intro_examples)
    - [Data transforms](/examples/data_transforms/intro_data_transforms)
    - [Math examples](/examples/maths/Big_prime)
  </Card>

</CardGrid>

## Why Knitting

- Function-call workflow instead of manual message routing.
- Lower IPC overhead for high-frequency workloads.
- Same model across Node, Deno, and Bun.
- Explicit control over batching, inlining, and shutdown behavior.


## Project Scope

Knitting focuses on local multi-thread execution with shared-memory IPC.
It is not a distributed scheduler or cluster manager.
