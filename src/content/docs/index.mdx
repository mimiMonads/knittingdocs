---
title: "Knitting"
description: "Knitting is shared-memory IPC for Node.js, Deno, and Bun, built for low-latency worker tasks and high-throughput parallel JavaScript."
template: splash
head:
  - tag: meta
    attrs:
      name: keywords
      content: "shared-memory IPC, Node.js worker threads, Deno workers, Bun workers, JavaScript parallelism, Knitting"
  - tag: meta
    attrs:
      property: og:type
      content: website
  - tag: meta
    attrs:
      name: robots
      content: "index,follow,max-image-preview:large"

hero:
  image:
    alt: A glittering, brightly colored logo
    dark: ../../assets/logo.svg
    light: ../../assets/light_logo.svg
  tagline: "Real nanosecond responses for JavaScript."
  actions:
    - text: Welcome
      link: /start/welcome/
    - text: Benchmarks
      link: /benchmarks/introduction/
---

import AshesBackground from "../../components/AshesBackground.astro";
import { Card, CardGrid, FileTree } from "@astrojs/starlight/components";
import { Tabs, TabItem } from "@astrojs/starlight/components";
import nodeIpc from "../../assets/charts/node_ipc.png";
import denoIpc from "../../assets/charts/deno_ipc.png";
import bunIpc from "../../assets/charts/bun_ipc.png";
import BenchmarkBalls from "../../components/BenchmarkBalls.astro";

<AshesBackground />

<BenchmarkBalls
  sourceLabel="based on node.js ipc graph (50 msg)"
  items={[
    { label: "Knitting", speed: 2030869, color: "#336244ff" },
    { label: "PostMessage", speed: 573394, color: "#58523bff" },
    { label: "WebSockets", speed: 178603, color: "#A78BFA" },
    { label: "HTTP", speed: 17483,  color: "#60A5FA" },
  ]}
/>
<div class="home-cards">
  <CardGrid stagger>
    <Card title="Multi-threading in 10 lines">
      Start here if you want the smallest possible mental model.
      Define tasks, create a pool, call it, and shut it down.

      ```ts
      import { isMain, task } from "@vixeny/knitting";

      export const world = task({
        f: (args: string) => args + " world",
      }).createPool({
        threads: 2,
      });

      if (isMain) {
        world.call("hello")
          .then(console.log)
          .finally(world.shutdown);
      }
      ```


      This is intentionally simple. You can add batching and balancing later.
    </Card>

    <Card title="Throughput">
    These results measure a full `1 MB` echo roundtrip: the host sends `1 MB` to a worker, and the worker sends `1 MB` back.
    
    Each call creates two `1 MB` objects and transfers `2 MB` total.

    To stay conservative, we report one-way equivalent throughput (half of the roundtrip GB/s).

    Even with this strict measure, Bun reaches double-digit GB/s.

      **Hot scoreboard (GB/s, one-way equivalent):**

      | Runtime | `Uint8Array` | `string` |
      | --- | ---: | ---: |
      | **Bun** | **11.93** | **9.74** |
      | **Deno** | **1.17** | **2.68** |
      | **Node** | **1.15** | **1.32** |


      [Benchmarks overview](/benchmarks/introduction/)
   
    </Card>

    <Card title="How fast?" >
      A quick look at runtime behavior across Bun, Deno, and Node.
      Switch tabs to compare how each runtime responds under the same benchmark shape.

      <Tabs>
        <TabItem label="Node" icon="node">
          <img
            src={nodeIpc.src}
            width={nodeIpc.width}
            height={nodeIpc.height}
            alt="Node IPC benchmark"
            loading="lazy"
          />
        </TabItem>
        <TabItem label="Bun" icon="bun">
          <img
            src={bunIpc.src}
            width={bunIpc.width}
            height={bunIpc.height}
            alt="Bun IPC benchmark"
            loading="lazy"
          />
        </TabItem>
        <TabItem label="Deno" icon="deno">
          <img
            src={denoIpc.src}
            width={denoIpc.width}
            height={denoIpc.height}
            alt="Deno IPC benchmark"
            loading="lazy"
          />
        </TabItem>
      </Tabs>

      Use this as orientation, then check the benchmark pages for details.
    </Card>





    <Card title="One more thread, big impact">
      The simplest win: move the "heavy" part of your route (SSR, parsing, crypto, compression) into a worker
      pool so the main request loop stays snappy.

      In the Hono server example, one run shows:
      - SSR-only throughput: **~+90%** (about **1.9x**).
      - Mixed load (ping + jwt + ssr together): `/ping` **~+59%**, `/jwt` **~+57%**, `/ssr` **~+58%**.

      Why it works: without workers, an expensive request can monopolize the main thread and create
      head-of-line blocking. Adding even a single worker thread lets that heavy work run in parallel,
      so cheap routes keep flowing and tail latency stops spiking.

      Try it in the Hono server example:
      - [Hono + SSR + JWT + Zod](/examples/data_transforms/rendering_output/Hono_server)
    </Card>

        <Card title="10+ diverse examples (and growing)">
      There is enough here to copy real patterns, not just toy snippets.
      We keep adding examples as new workflows land.

      <FileTree>
      - examples/
        - intro_examples.mdx
        - data_transforms/
          - intro_data_transforms.mdx
          - rendering_output/
            - React_ssr.mdx
            - Hono_server.mdx
            - …
          - validation/
            - Schema_validate.mdx
            - Jwt_revalidation.mdx
            - …
        - maths/
          - Big_prime.mdx
          - Monte_pi.mdx
          - Physics_loop.mdx
          - …
      </FileTree>

      - [Browse examples](/examples/intro_examples)

    We are fully documentated!
    </Card>

    <Card title="Flexible by default">
      Built to keep agent-style workflows simple and adaptable.

      - Promise inputs and async outputs work naturally.
      - Rich payload support plus explicit serialization behavior..
      - Uses `seralize` if something is not in the list.  

      **Knitting exposes the knobs you usually need in production.**

      - Balance lanes with `balancer` strategies.
      - Add host execution with `inliner` when it helps.
      - Control `spin`/`park`/`backoff`/`timeout` and more.

    </Card>

  </CardGrid>
</div>


## Why Knitting

- Function-call workflow instead of manual message routing.
- Lower IPC overhead for high-frequency workloads.
- Same model across Node, Deno, and Bun.
- Explicit control over batching, inlining, and shutdown behavior.


## Project Scope

Knitting focuses on local multi-thread execution with shared-memory IPC.
It is not a distributed scheduler or cluster manager.
