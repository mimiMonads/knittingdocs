---
title: Welcome
description: What Knitting is and how it works.
sidebar:
  order: 1
---
import { Tabs, TabItem } from '@astrojs/starlight/components';
import { Code } from '@astrojs/starlight/components';
import { getCode } from '../../../lib/code-snippets';
import nodeBench from '../../../assets/results/node_ipc.md?raw';
import nodeIpc from '../../../assets/charts/node_ipc.png';
import denoBench from '../../../assets/results/deno_ipc.md?raw';
import denoIpc from '../../../assets/charts/deno_ipc.png';
import bunBench from '../../../assets/results/bun_ipc.md?raw';
import bunIpc from '../../../assets/charts/bun_ipc.png';

export const hello = getCode('hello_world.ts');

Welcome! We’re glad you’re here.

This project is for anyone who wants JavaScript to scale across cores without
giving up simplicity, and who wants `multi-core JavaScript` to be viable in
spaces traditionally dominated by Rust and Go.

Let’s dive in.

---

## Why Knitting Exists

JavaScript can do many things at once, but it can’t run CPU work in parallel on a single thread. That’s why **Workers** exist.

In general application code, though, workers are still surprisingly rare. Not because parallelism isn’t useful, but because the default solutions drifted elsewhere.

Promises made I/O concurrency painless. When servers got hot, it often felt
easier to scale out: spawn another process or container and communicate over
HTTP or WebSockets.

That approach works, but it comes with a cost:

- more `memory`,
- more `moving pieces`,
- more `latency`,
- and an entire communication layer that has nothing to do with your `actual logic`.

Workers `should` be the middle ground, yet the typical workflow is still
friction-heavy:

- `create` a separate worker entry module,
- `build` an event protocol on both sides,
- `implement` routing and error handling,
- `wrap` everything in promises,
- `track` request IDs (and ordering, if FIFO matters),
- then discover that IPC overhead makes many small tasks `not worth` offloading.

None of this is conceptually hard. It’s just a lot of machinery for something that should feel like calling a function.

> The problem isn’t compute. The problem is the cost of reaching the compute.

With **Knitting**, you can do that `in about ten lines`:

<Code code={hello} lang="ts" title={"hello_world.ts"} />

Knitting exists to make `“parallelize this”` practical again:

- `define` tasks once,
- `spin up` a pool,
- `call` them like functions.

The result is a worker workflow that just works and can be **4x to 200x
faster** than message-based IPC approaches for small-to-medium,
high-frequency tasks.

<Tabs>
  <TabItem label="Node">
    <img
      src={nodeIpc.src}
      width={nodeIpc.width}
      height={nodeIpc.height}
      alt="Node IPC benchmark"
      loading="lazy"
    />
  </TabItem>
  <TabItem label="Node data">
    <Code code={nodeBench} lang="md" title={"node_test.md"} />
  </TabItem>
  <TabItem label="Deno">
    <img
      src={denoIpc.src}
      width={denoIpc.width}
      height={denoIpc.height}
      alt="Deno IPC benchmark"
      loading="lazy"
    />
  </TabItem>
  <TabItem label="Deno data">
    <Code code={denoBench} lang="md" title={"deno_test.md"} />
  </TabItem>
  <TabItem label="Bun">
    <img
      src={bunIpc.src}
      width={bunIpc.width}
      height={bunIpc.height}
      alt="Bun IPC benchmark"
      loading="lazy"
    />
  </TabItem>
  <TabItem label="Bun data">
    <Code code={bunBench} lang="md" title={"bun_test.md"} />
  </TabItem>
</Tabs>

---

## What Knitting Is

Knitting is a shared-memory IPC transport for JavaScript runtimes that keeps things simple.

If you prefer the long version:

> Knitting is a full-duplex shared-memory IPC transport built from two independent 32-slot shared mailboxes (request and response), using bitset slot ownership and futex-style wakeups with optional spin-pause.

At a higher level, the design rests on a few core ideas.

### Core Design Ideas

**Tasks, not messages**  
You export tasks as functions, and the pool executes them. You don’t build your application around low-level message protocols.

**Configurable policy**  
Routing strategy, batching limits, stall backoff, timeouts, and inline execution are explicit choices, not hidden behavior.

**Workers don’t cook the CPU**  
Workers spin briefly when needed, then park and wake up only when there’s work to do.

---

## Why It’s Fast (and When That Matters)

Knitting keeps the management cost of parallelism low. The speedups do not come
from magic; they come from removing friction:

- **Low IPC overhead:** shared-memory mailboxes avoid serialization and message hops.
- **Batch-friendly execution:** work is encoded, dispatched, and resolved through a pipelined flow.
- **Sleep instead of spin:** workers park and wake only when there’s work.
- **Event-loop friendly:** runtimes cooperate with the event loop: no blocking,
  no forced `await`, predictable behavior.
- **Pressure-responsive:** higher load improves throughput instead of stalling.

This matters most when the cost of reaching the compute is comparable to the compute itself.

> Knitting doesn’t try to make slow code fast, it tries to make parallel code cheap enough to be worth writing.

---

## What Knitting Is Good For

Knitting shines in workloads that mix CPU pressure with burstiness:

- **Math and simulation:** [Big prime](/examples/maths/big_prime), [Monte Carlo
  π](/examples/maths/monte_pi), [Physics loop](/examples/maths/physics_loop),
  [TSP (GSA)](/examples/maths/tsp_gsa).
- **Data transforms:** [Overview](/examples/data_transforms/intro_data_transforms),
  [Schema validation](/examples/data_transforms/validation/schema_validate),
  [Prompt token budgeting](/examples/data_transforms/validation/prompt_token_budgeting),
  [React SSR parsing](/examples/data_transforms/rendering_output/react_ssr),
  [React SSR compression](/examples/data_transforms/rendering_output/react_ssr_compress),
  [Markdown to HTML](/examples/data_transforms/rendering_output/markdown_to_html).
- **Web servers with CPU spikes:** keeping the main thread responsive under load.
- **High-frequency background jobs:** lots of small tasks where batching pays off.

---

## Design Boundaries & Non-Goals

Knitting is `opinionated` by design. It solves a very specific problem well, and it intentionally does not try to be everything.

Being explicit about this matters.

**Not a replacement for `postMessage`**  
Knitting does not aim to replace `postMessage`. `postMessage` is excellent for general-purpose messaging. Knitting focuses on task execution with low overhead.

Also, we still need to implement:
* Barriers
* Cross-thread values
* Signals

There is still a lot to do, and that is okay!

**No browser / Web Worker support (intentionally)**  
Knitting does not currently target browser Web Workers. This is not because it
is technically difficult; with the current architecture, a web version would be
fairly straightforward.

The real reason is security. Shared memory in the browser comes with strict
constraints, and speculative execution attacks have shown these risks are real
(`yes, Spectre`). Until we know it is safe, Knitting stays server-side by
design.

**No edge runtimes (for now)**  
We currently target Node.js, Deno, and Bun. Edge runtimes and cloud workers
have different isolation models, limited or absent shared memory, and different
module lifecycles.

Supporting them properly would require careful design, not a quick port.

**Remote imports are treated with caution**  
Knitting relies on loading task modules consistently across workers. Importing
code directly from the network introduces security risks, reproducibility
issues, and unclear trust boundaries.

If remote imports are ever supported, they will likely appear first in `Deno`,
be opt-in, and come with explicit safety constraints.

**Not a distributed system**  
Knitting is not a cluster manager, a job queue, or a distributed execution framework. It operates within a single runtime instance, across threads, using shared memory. If you need cross-machine distribution, Knitting is meant to complement that layer, not replace it.

Theoretically, because we use shared memory, we could have real IPC with other
languages, but that is a far-off dream. There is still a lot to build first.

---

## Thanks

Thanks for reading this far.  
It’s always a little sad to reach the end, but now the fun part starts.
