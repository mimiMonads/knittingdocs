---
title: Deno
description: Deno benchmark results for Knitting, including IPC latency, worker comparison, heavy-load scaling, and payload type costs.
sidebar:
  order: 3
---

import { Tabs, TabItem } from '@astrojs/starlight/components';
import { Code } from '@astrojs/starlight/components';
import denoIpcBench from '../../../assets/results/deno_ipc.md?raw';
import denoIpc from '../../../assets/charts/deno_ipc.png';
import denoTypesBench from '../../../assets/results/deno_types.md?raw';
import denoTypesOne from '../../../assets/charts/deno_types_types_count1.png';
import denoTypesHundred from '../../../assets/charts/deno_types_types_count100.png';
import denoHeavyLoad from '../../../assets/results/deno_withload.md?raw';
import denoAllTypes from '../../../assets/results/deno_types_knitting.md?raw';
import HeavyTaskPrimeTransferCaution from '../../../components/tips/HeavyTaskPrimeTransferCaution.astro';

This page summarizes Deno benchmark runs for Knitting on `deno 2.6.6 (aarch64-apple-darwin)`.

## IPC (Deno)
This benchmark compares one round-trip between a main thread and workers using different transports.
Knitting keeps the lowest overhead in this setup:

- `1` message: Knitting is about `3.5x` faster than worker `postMessage`, `3.6x` faster than websocket, and `10x` faster than HTTP.
- `25` messages: Knitting is about `9.5x` faster than worker `postMessage`.
- `50` messages: Knitting is about `10.7x` faster than worker `postMessage`.

### Data
<Tabs>
  <TabItem label="Deno">
    <img
      src={denoIpc.src}
      width={denoIpc.width}
      height={denoIpc.height}
      alt="Deno IPC benchmark"
      loading="lazy"
    />
  </TabItem>
  <TabItem label="Raw data">
    <Code code={denoIpcBench} lang="md" title={"deno_ipc.md"} />
  </TabItem>
</Tabs>

## Knitting vs Worker (Deno)
These charts compare the same payload families sent through Knitting and Deno workers.

### One message
With a single value per call, Knitting is consistently faster:

- For small primitives, Knitting is roughly `25-30x` faster than workers.
- For string/array/object payloads, Knitting is usually around `4-6x` faster.
- For larger payloads, Knitting still holds a clear advantage (for example, big object: `6.37 µs` vs `27.45 µs`, about `4.3x` faster).

<Tabs>
  <TabItem label="Knitting vs Worker (1)">
    <img
      src={denoTypesOne.src}
      width={denoTypesOne.width}
      height={denoTypesOne.height}
      alt="Deno knitting vs worker benchmark 1"
      loading="lazy"
    />
  </TabItem>
  <TabItem label="Data">
    <Code code={denoTypesBench} lang="md" title={"deno_types.md"} />
  </TabItem>
</Tabs>

### 100 messages
At `100` messages per iteration, the gap remains strong:

- Typical primitives stay around `18-35x` faster with Knitting.
- Heavier payloads still keep a clear edge at roughly `~7x` faster.
- Batching improves throughput for both, but Knitting remains lower-overhead across payload classes.

<Tabs>
  <TabItem label="Knitting vs Worker (100)">
    <img
      src={denoTypesHundred.src}
      width={denoTypesHundred.width}
      height={denoTypesHundred.height}
      alt="Deno knitting vs worker benchmark 100"
      loading="lazy"
    />
  </TabItem>
  <TabItem label="Data">
    <Code code={denoTypesBench} lang="md" title={"deno_types.md"} />
  </TabItem>
</Tabs>

## Efficiency under heavy tasks (Deno)
<HeavyTaskPrimeTransferCaution />

This stress test computes prime numbers over a large range, then serializes and parses large JSON payloads:

```ts
const N = 10_000_000; // search range: [1..N]
const CHUNK_SIZE = 250_000;
```

Even under this heavier workload, parallel workers scale well:

- `main + 1 extra thread`: `~1.8x` faster than main only.
- `main + 2 extra threads`: `~2.5x` faster than main only.
- `main + 3 extra threads`: `~3.2x` faster than main only.
- `main + 4 extra threads`: `~3.7x` faster than main only.

### Data
<Code code={denoHeavyLoad} lang="md" title={"deno_withload.md"} />

## All types in Knitting
This benchmark covers primitive, structured, collection, typed-array, error/date/symbol, promise-arg, and static-vs-dynamic allocator paths.
Results are reported for count `1` and count `100` to show both per-call latency and batched throughput.

Quick takeaways:

- In count `100`, primitive-style payloads are usually in the `~20-50 µs` range, while heavier structured/collection payloads can be `~160 µs` to multi-millisecond outliers.
- The static payload path is usually around `~1.8x-2.5x` faster than dynamic allocator paths (for example: string `~2.2x`, json `~2.5x`, `Uint8Array` `~1.8x`, symbol `~2.2x` at count `100`).

Payload sizes (approximate):

| Payload | Size |
| --- | ---: |
| `jsonObj` | `206 B` |
| `jsonArr` | `217 B` |
| `mapPayload` | `284 B` |
| `Uint8Array` | `1024 B` |
| `Int32Array` | `1024 B` |
| `Float64Array` | `1024 B` |
| `BigInt64Array` | `1024 B` |
| `BigUint64Array` | `1024 B` |
| `DataView` | `1024 B` |
| `smallU8` | `480 B` |
| `largeU8` | `481 B` |

### Data
<Code code={denoAllTypes} lang="md" title={"deno_types_knitting.md"} />
