---
title: Bun
description: Bun benchmark results for Knitting, including IPC latency, worker comparison, heavy-load scaling, and payload type costs.
sidebar:
  order: 4
---

import { Tabs, TabItem } from '@astrojs/starlight/components';
import { Code } from '@astrojs/starlight/components';
import bunIpcBench from '../../../assets/results/bun_ipc.md?raw';
import bunIpc from '../../../assets/charts/bun_ipc.png';
import bunTypesBench from '../../../assets/results/bun_types.md?raw';
import bunTypesOne from '../../../assets/charts/bun_types_types_count1.png';
import bunTypesHundred from '../../../assets/charts/bun_types_types_count100.png';
import bunHeavyLoad from '../../../assets/results/bun_withload.md?raw';
import bunAllTypes from '../../../assets/results/bun_types_knitting.md?raw';

This page summarizes Bun benchmark runs for Knitting on `bun 1.3.6 (arm64-darwin)`.

## IPC (Bun)
This benchmark compares one round-trip between a main thread and workers using different transports.
Knitting keeps the lowest overhead in this setup:

- `1` message: Knitting is about `5.7x` faster than worker `postMessage`, `9.5x` faster than websocket, and `21x` faster than HTTP.
- `25` messages: Knitting is about `7.4x` faster than worker `postMessage`.
- `50` messages: Knitting is about `7x` faster than worker `postMessage`.

### Data
<Tabs>
  <TabItem label="Bun">
    <img
      src={bunIpc.src}
      width={bunIpc.width}
      height={bunIpc.height}
      alt="Bun IPC benchmark"
      loading="lazy"
    />
  </TabItem>
  <TabItem label="Raw data">
    <Code code={bunIpcBench} lang="md" title={"bun_ipc.md"} />
  </TabItem>
</Tabs>

## Knitting vs Worker (Bun)
These charts compare the same payload families sent through Knitting and Bun workers.

### One message
With a single value per call, Knitting is consistently faster:

- For small primitives, Knitting is roughly `~6-17x` faster than workers.
- For string/array/object payloads, Knitting is usually around `~4-6x` faster.
- For larger payloads, Knitting still holds a clear advantage (for example, big object: `3.97 µs` vs `16.38 µs`, about `4.1x` faster).

<Tabs>
  <TabItem label="Knitting vs Worker (1)">
    <img
      src={bunTypesOne.src}
      width={bunTypesOne.width}
      height={bunTypesOne.height}
      alt="Bun knitting vs worker benchmark 1"
      loading="lazy"
    />
  </TabItem>
  <TabItem label="Data">
    <Code code={bunTypesBench} lang="md" title={"bun_types.md"} />
  </TabItem>
</Tabs>

### 100 messages
At `100` messages per iteration, the gap remains strong:

- Typical primitives stay around `~7-9x` faster with Knitting.
- Heavier payloads still keep a clear edge at roughly `~3.7-3.9x` faster.
- Batching improves throughput for both, but Knitting remains lower-overhead across payload classes.

<Tabs>
  <TabItem label="Knitting vs Worker (100)">
    <img
      src={bunTypesHundred.src}
      width={bunTypesHundred.width}
      height={bunTypesHundred.height}
      alt="Bun knitting vs worker benchmark 100"
      loading="lazy"
    />
  </TabItem>
  <TabItem label="Data">
    <Code code={bunTypesBench} lang="md" title={"bun_types.md"} />
  </TabItem>
</Tabs>

## Efficiency under heavy tasks (Bun)
This stress test computes prime numbers over a large range, then serializes and parses large JSON payloads:

```ts
const N = 10_000_000; // search range: [1..N]
const CHUNK_SIZE = 250_000;
```

Even under this heavier workload, parallel workers scale well:

- `main + 1 extra thread`: `~1.8x` faster than main only.
- `main + 2 extra threads`: `~2.5x` faster than main only.
- `main + 3 extra threads`: `~3.3x` faster than main only.
- `main + 4 extra threads`: `~3.8x` faster than main only.

### Data
<Code code={bunHeavyLoad} lang="md" title={"bun_withload.md"} />

## All types in Knitting
This benchmark covers primitive, structured, collection, typed-array, error/date/symbol, promise-arg, and static-vs-dynamic allocator paths.
Results are reported for count `1` and count `100` to show both per-call latency and batched throughput.

Quick takeaways:

- In count `100`, primitive-style payloads are usually in the `~16-25 µs` range, while heavier structured/collection payloads are often `~60-270 µs` with occasional larger spikes.
- The static payload path is usually around `~1.3x-3.1x` faster than dynamic allocator paths (for example: string `~1.8x`, json `~2.1x`, `Uint8Array` `~1.3x`, symbol `~3.1x` at count `100`).

Payload sizes (approximate):

| Payload | Size |
| --- | ---: |
| `jsonObj` | `206 B` |
| `jsonArr` | `217 B` |
| `mapPayload` | `284 B` |
| `Uint8Array` | `1024 B` |
| `Int32Array` | `1024 B` |
| `Float64Array` | `1024 B` |
| `BigInt64Array` | `1024 B` |
| `BigUint64Array` | `1024 B` |
| `DataView` | `1024 B` |
| `smallU8` | `480 B` |
| `largeU8` | `481 B` |

### Data
<Code code={bunAllTypes} lang="md" title={"bun_types_knitting.md"} />
