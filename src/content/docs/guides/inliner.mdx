---
title: Inliner
description: Run tasks on the main thread as an extra lane.
sidebar:
  order: 3
---

The `inliner` option adds the host (main thread) as one extra lane in the pool.
It helps distribute queued work across workers plus the host, instead of sending
everything to workers only.

```ts
const pool = createPool({
  threads: 4,
  inliner: { position: "last" },
})({ add });
```

:::note
The inliner is a lane, not a replacement for worker threads.
Use it to complement workers, not to avoid them.
:::

## Why It Exists

- It can reduce worker pressure by letting the host participate in execution.
- It can improve throughput for compute-style workloads when tasks are short and
  plentiful.

## How It Runs

Inline execution is not immediate in the `call.*()` path.
Calls are queued, then processed when the macro queue turn runs.

This delay is intentional: by that point, the dispatcher has had a chance to
send/receive worker tasks first, then the host drains inline work.

`batchSize` controls how many inline tasks run per macro-queue turn.

:::warning
Inline tasks run on the main thread. CPU-heavy work will block the event loop
(timers, I/O callbacks, HTTP handlers, etc.).
The macro-queue loop is useful, but not free.
:::

## Options

```ts
createPool({
  threads: number,
  inliner: {
    position?: "first" | "last",
    batchSize?: number,
    dispatchThreshold?: number,
  },
  balancer?: "robinRound" | "firstIdle" | "randomLane" | "firstIdleOrRandom",
})
```

### position

Controls where the inline lane appears relative to worker lanes.

- `"first"`: host lane is considered early.
- `"last"`: host lane is considered after workers.

For event-loop sensitive workloads (for example HTTP), `"last"` is usually safer.

### batchSize

Limits how many inline tasks are processed per macro-queue turn.

- Higher values are often good for math/compute throughput.
- Lower values are usually better for HTTP/event-loop responsiveness.

If not set, it defaults to `1` when the inliner is enabled.

### dispatchThreshold

Controls when the inline lane becomes eligible for scheduling.

`dispatchThreshold` was added mainly for event-loop sensitive producers (HTTP/I/O)
that can flood a pool with calls. During a traffic spike, the host is already
busy doing the plumbing: deciding where each call should run and
encoding/decoding payloads to communicate with workers.

A higher `dispatchThreshold` keeps the host lane out of scheduling at light
concurrency, then lets it join only once there is a meaningful burst in flight.
This acts like a pressure-relief valve: the extra lane helps drain backlog, and
inline executions avoid worker IPC overhead (no worker encode/decode), which can
leave more room for the event loop and other pools to make progress.

- `1` (default): inline lane is immediately eligible.
- Higher values: host lane stays excluded until concurrency rises.
- If `inliner` is not enabled, this option has no effect.


## Exact Behavior

- The scheduler tracks `inFlight` calls per task invoker.
- On each call, `inFlight` increments before lane selection.
- If `inFlight < dispatchThreshold`, scheduling uses worker-only lanes (inline
  lane excluded).
- If `inFlight >= dispatchThreshold`, scheduling uses all lanes (workers +
  inline lane).
- `inFlight` decrements on resolve, reject, or synchronous throw.
- The configured balancer strategy (`robinRound`, `firstIdle`, `randomLane`,
  `firstIdleOrRandom`) applies to whichever lane set is currently active.

## Behavior Checks

Existing tests validate both cases:

- Below threshold: inline lane stays unused.
- At/above threshold: inline lane becomes eligible and is used.

## Balancer Guidance

- For HTTP-style workloads:
  use `inliner.position: "last"` with `balancer: "firstIdle"` so workers are
  prioritized before the host lane. Add a higher `dispatchThreshold` to keep
  host scheduling out of low-load traffic.
- For pools with many registered tasks or uneven load:
  `randomLane` or `firstIdleOrRandom` can help spread pressure.

## When To Use It

- Compute-heavy and batch-oriented workloads (math pipelines, simulations).
- Bursty queues where one extra lane helps drain work.
- Tasks that are short enough that host participation is worth it.

## When Not To Use It

- Request/response HTTP hot paths where event-loop latency matters.
- Blocking tasks that can delay timers, sockets, or callbacks on the host.
- Cases where isolation is more important than squeezing one extra lane.

For many HTTP systems, separate pools per workload type is usually a better
choice than aggressive inlining.


## Examples

### Compute-oriented

```ts
const { call, shutdown } = createPool({
  threads: 4,
  inliner: { position: "last", batchSize: 20 },
  balancer: "firstIdleOrRandom",
})({ scoreChunk });
```

### HTTP-friendly fallback

```ts
const { call, shutdown } = createPool({
  threads: 2,
  inliner: {
    position: "last",
    dispatchThreshold: 16,
  },
  balancer: "firstIdle",
})({ routeTask });
```
